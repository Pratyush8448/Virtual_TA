## Vision Models

[![LLM Vision Models](https://i.ytimg.com/vi_webp/FgT_Mk_bakQ/sddefault.webp)](https://youtu.be/FgT_Mk_bakQ)

You'll learn how to use LLMs to interpret images and extract useful information, covering:

- **Setting Up Vision Models**: Integrate vision capabilities with LLMs using APIs like OpenAI's Chat Completion.
- **Sending Image URLs for Analysis**: Pass URLs or base64-encoded images to LLMs for processing.
- **Reading Image Responses**: Get detailed textual descriptions of images, from scenic landscapes to specific objects like cricketers or bank statements.
- **Extracting Data from Images**: Convert extracted image data to various formats like Markdown tables or JSON arrays.
- **Handling Model Hallucinations**: Address inaccuracies in extraction results, understanding how different prompts can affect output quality.
- **Cost Management for Vision Models**: Adjust detail settings (e.g., "detail: low") to balance cost and output precision.

Here are the links used in the video:

- [Jupyter Notebook](https://colab.research.google.com/drive/1bK0b1XMrZWImtw01T1w9NGraDkiVi8mS)
- [OpenAI Chat API Reference](https://platform.openai.com/docs/api-reference/chat/create)
- [OpenAI Vision Guide](https://platform.openai.com/docs/guides/vision)
- [Sample images used](https://drive.google.com/drive/folders/14MFc7XmGIUDU4-vbmF9305c1SSQrM-gR)

Here is an example of how to analyze an image using the OpenAI API.

```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "What is in this image?"},
          {
            "type": "image_url",
            "detail": "low",
            "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png"}
          }
        ]
      }
    ]
  }'
```

Let's break down the request: